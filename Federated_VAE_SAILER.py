# -*- coding: utf-8 -*-
"""SAILER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PPDPTn7BfcFy4CJi6gU-N7GRh5-f_m82
"""

from google.colab import drive
drive.mount('/gdrive',force_remount=True)

# Commented out IPython magic to ensure Python compatibility.
# %cd /gdrive/MyDrive/Federated Learning/SAILER-main
!ls

#%load dataset.py
from dataset import *
from trainer import Trainer

!pip install ezdict

!pip install umap-learn datashader bokeh holoviews

import sys
from ezdict import EZDict
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import umap
import matplotlib
import pandas as pd
from sklearn.feature_selection import mutual_info_regression as MIR

def get_cmap(n, name='tab10'):
    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct 
    RGB color; the keyword argument name must be a standard mpl colormap name.'''
    return plt.cm.get_cmap(name, n)

cmap = get_cmap(10)

"""## Data Files
Files are named after simulation setting, signal to noise ratio and mean number of peaks per cell. Data are stored as sparse matrices in npz files. Cell labels are in txt files.
"""

!ls ./data/SimATAC

"""## Training"""

args = EZDict({
    "name": 'sim1_z10',
    'log': 'train_log.csv',
    'load_ckpt': False, 
    'cuda_dev': [0], #False
    'sample_batch': False,
    "max_epoch": 100,
    'start_epoch': 1,
    'batch_size': 100,
    'start_save': 98,
    'LAMBDA': 1,
    'conv': False,
    'model_type': 'inv',
    'data_type': 'simATAC',
    'lr': 1e-3, 
    'pos_w': 20,
    'weight_decay': 5e-4,
    'optimizer': 'adam',
    'z_dim': 10,
    'out_every': 20,
    'ckpt_dir': './models/',
    #simATAC
    'setting': 2,
    'signal': 0.4,
    'frags': 5000,
    'bin_size': 10000,
    #ATACbenchmark
    'cov': 1000,
    'clients':2,
    'file_name':"/gdrive/MyDrive/Federated Learning/Data/Varying_depth_Varying_rou/High.mean_10000/peakMat.VarLen.3000_2500_2000_1500_1000.rou0.8.mean10000.sd1.5.npz",
    'type_name':"/gdrive/MyDrive/Federated Learning/Data/Varying_depth_Varying_rou/High.mean_10000/cellLabel.VarLen.3000_2500_2000_1500_1000.rou0.8.mean10000.sd1.5.txt"
})

solver = Trainer(args)

solver.warm_up()

solver.inv_train()

"""## Cyclical Annealing Schedule

See [here](https://www.microsoft.com/en-us/research/blog/less-pain-more-gain-a-simple-method-for-vae-training-with-less-of-that-kl-vanishing-agony/) for a blog on how to deal with kl vanishing.
![image](https://www.microsoft.com/en-us/research/uploads/prod/2019/04/equation.png)

### (a) monotonic schedule vs (b) the cyclical schedule
![image](https://www.microsoft.com/en-us/research/uploads/prod/2019/04/Annealing-with-the-monotonic-schedule-768x498.png)
### Example results
![image](https://www.microsoft.com/en-us/research/uploads/prod/2019/04/figure-4-1.png)
![image](https://www.microsoft.com/en-us/research/uploads/prod/2019/04/figure-4.png)

## Traing log
"""

log = pd.read_csv('models/sim1_z10/invtrain_log.csv')

log

plt.plot(log.kl[100:])

plt.plot(log.bce)

"""## Visualize latent embedding
Visualize latent embedding of cells with UMAP. Cells are colored by ground truth cell types.
"""

latent, labels, depth = solver.encode_adv()

result = latent.numpy()

l = pd.DataFrame(labels, columns=['celltype'])

reducer = umap.UMAP(random_state=123)
X_embedded = reducer.fit_transform(result)

plt.figure(figsize=(8, 8))
for i, c in enumerate(np.unique(l)):
    mask = (l == c).values.flatten()
    plt.scatter(X_embedded[mask, 0], X_embedded[mask, 1], label=c, s=3, color=cmap(i))
    plt.xticks([], [])
    plt.yticks([], [])
plt.tight_layout()
plt.legend()

"""## Visualize depth distribution on latent embedding"""

fig = plt.figure(figsize=(8, 8))
plt.scatter(X_embedded[:, 0], 
            X_embedded[:, 1], 
            s=1, 
            c=np.array(depth),
            norm=matplotlib.colors.LogNorm(),
            cmap='pink'
            )
plt.xticks([], [])
plt.yticks([], [])
cbar = plt.colorbar()
cbar.remove()
plt.tight_layout()

"""## Mutual Information
Caculate mean mutual information between latent embedding and read depth.
"""

mi = MIR(result, depth)
print(mi.mean())

